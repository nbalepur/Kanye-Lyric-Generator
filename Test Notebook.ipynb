{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"kanye_verses.txt\", \"r\", encoding = \"utf8\")\n",
    "text = file.read()\n",
    "text = text.replace(\"\\n\\n\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lyric(txt):\n",
    "    return re.sub(\"[^a-z' ]\", \"\", txt).replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = text.lower().split(\"\\n\")\n",
    "lyrics = np.unique(lyrics)[1:].tolist()\n",
    "cleaned_lyrics = [clean_lyric(lyric) for lyric in lyrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(lyric, seq_len):\n",
    "    sequences = []    \n",
    "    if len(lyric.split()) <= seq_len:\n",
    "        return [lyric]\n",
    "    \n",
    "    for itr in range(seq_len, len(lyric.split())):\n",
    "        curr_seq = lyric.split()[itr - seq_len:itr + 1]\n",
    "        sequences.append(\" \".join(curr_seq))\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sequences = [create_sequences(cleaned_lyric, 2) for cleaned_lyric in cleaned_lyrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.unique(np.array(sum(raw_sequences, []))).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_words = np.unique(np.array(\" \".join(sequences).split(\" \")))\n",
    "uniq_words_idx = np.arange(uniq_words.size)\n",
    "\n",
    "word_to_idx = dict(zip(uniq_words.tolist(), uniq_words_idx.tolist()))\n",
    "idx_to_word = dict(zip(uniq_words_idx.tolist(), uniq_words.tolist()))\n",
    "\n",
    "vocab_size = len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_word = []\n",
    "y_word = []\n",
    "\n",
    "for seq in sequences:\n",
    "    \n",
    "    if (len(seq.split()) != 3):\n",
    "        continue\n",
    "    \n",
    "    x_word.append(\" \".join(seq.split()[:-1]))\n",
    "    y_word.append(\" \".join(seq.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_idx(seq):\n",
    "    return [word_to_idx[word] for word in seq.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_idx = np.array([get_seq_idx(word) for word in x_word])\n",
    "y_idx = np.array([get_seq_idx(word) for word in y_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch(x, y, batch_size):\n",
    "    \n",
    "    for itr in range(batch_size, x.shape[0], batch_size):\n",
    "        curr_x = x[itr - batch_size:itr, :]\n",
    "        curr_y = y[itr - batch_size:itr, :]\n",
    "        \n",
    "        yield curr_x, curr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
    "\n",
    "        ## define the LSTM\n",
    "        self.lstm = nn.LSTM(200, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## define the fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "\n",
    "        ## pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        ## pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        #out = out.contiguous().view(-1, self.n_hidden) \n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        ## put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        # if GPU is available\n",
    "        if (torch.cuda.is_available()):\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        \n",
    "        # if GPU is not available\n",
    "        else:\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 256\n",
    "num_layers = 4\n",
    "embed_size = 200\n",
    "drop_prob = 0.3\n",
    "lr = 0.001\n",
    "num_epochs = 15\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WordLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
    "    \n",
    "    # optimizer\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # push model to GPU\n",
    "    #net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_next_batch(x_idx, y_idx, batch_size):\n",
    "            counter += 1\n",
    "            \n",
    "            # convert numpy arrays to PyTorch arrays\n",
    "            inputs, targets = torch.from_numpy(x).type(torch.LongTensor), torch.from_numpy(y).type(torch.LongTensor)\n",
    "            \n",
    "            # push tensors to GPU\n",
    "            #inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # detach hidden states\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(-1))\n",
    "\n",
    "            # back-propagate error\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            # update weigths\n",
    "            opt.step()            \n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "            \n",
    "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                    \"Step: {}...\".format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 256...\n",
      "Epoch: 1/20... Step: 512...\n",
      "Epoch: 1/20... Step: 768...\n",
      "Epoch: 1/20... Step: 1024...\n",
      "Epoch: 2/20... Step: 1280...\n",
      "Epoch: 2/20... Step: 1536...\n",
      "Epoch: 2/20... Step: 1792...\n",
      "Epoch: 2/20... Step: 2048...\n",
      "Epoch: 3/20... Step: 2304...\n",
      "Epoch: 3/20... Step: 2560...\n",
      "Epoch: 3/20... Step: 2816...\n",
      "Epoch: 3/20... Step: 3072...\n",
      "Epoch: 4/20... Step: 3328...\n",
      "Epoch: 4/20... Step: 3584...\n",
      "Epoch: 4/20... Step: 3840...\n",
      "Epoch: 4/20... Step: 4096...\n",
      "Epoch: 5/20... Step: 4352...\n",
      "Epoch: 5/20... Step: 4608...\n",
      "Epoch: 5/20... Step: 4864...\n",
      "Epoch: 5/20... Step: 5120...\n",
      "Epoch: 5/20... Step: 5376...\n",
      "Epoch: 6/20... Step: 5632...\n",
      "Epoch: 6/20... Step: 5888...\n",
      "Epoch: 6/20... Step: 6144...\n",
      "Epoch: 6/20... Step: 6400...\n",
      "Epoch: 7/20... Step: 6656...\n",
      "Epoch: 7/20... Step: 6912...\n",
      "Epoch: 7/20... Step: 7168...\n",
      "Epoch: 7/20... Step: 7424...\n",
      "Epoch: 8/20... Step: 7680...\n",
      "Epoch: 8/20... Step: 7936...\n",
      "Epoch: 8/20... Step: 8192...\n",
      "Epoch: 8/20... Step: 8448...\n",
      "Epoch: 9/20... Step: 8704...\n",
      "Epoch: 9/20... Step: 8960...\n",
      "Epoch: 9/20... Step: 9216...\n",
      "Epoch: 9/20... Step: 9472...\n",
      "Epoch: 9/20... Step: 9728...\n",
      "Epoch: 10/20... Step: 9984...\n",
      "Epoch: 10/20... Step: 10240...\n",
      "Epoch: 10/20... Step: 10496...\n",
      "Epoch: 10/20... Step: 10752...\n",
      "Epoch: 11/20... Step: 11008...\n",
      "Epoch: 11/20... Step: 11264...\n",
      "Epoch: 11/20... Step: 11520...\n",
      "Epoch: 11/20... Step: 11776...\n",
      "Epoch: 12/20... Step: 12032...\n",
      "Epoch: 12/20... Step: 12288...\n",
      "Epoch: 12/20... Step: 12544...\n",
      "Epoch: 12/20... Step: 12800...\n",
      "Epoch: 13/20... Step: 13056...\n",
      "Epoch: 13/20... Step: 13312...\n",
      "Epoch: 13/20... Step: 13568...\n",
      "Epoch: 13/20... Step: 13824...\n",
      "Epoch: 14/20... Step: 14080...\n",
      "Epoch: 14/20... Step: 14336...\n",
      "Epoch: 14/20... Step: 14592...\n",
      "Epoch: 14/20... Step: 14848...\n",
      "Epoch: 14/20... Step: 15104...\n",
      "Epoch: 15/20... Step: 15360...\n",
      "Epoch: 15/20... Step: 15616...\n",
      "Epoch: 15/20... Step: 15872...\n",
      "Epoch: 15/20... Step: 16128...\n",
      "Epoch: 16/20... Step: 16384...\n",
      "Epoch: 16/20... Step: 16640...\n",
      "Epoch: 16/20... Step: 16896...\n",
      "Epoch: 16/20... Step: 17152...\n",
      "Epoch: 17/20... Step: 17408...\n",
      "Epoch: 17/20... Step: 17664...\n",
      "Epoch: 17/20... Step: 17920...\n",
      "Epoch: 17/20... Step: 18176...\n",
      "Epoch: 18/20... Step: 18432...\n",
      "Epoch: 18/20... Step: 18688...\n",
      "Epoch: 18/20... Step: 18944...\n",
      "Epoch: 18/20... Step: 19200...\n",
      "Epoch: 18/20... Step: 19456...\n",
      "Epoch: 19/20... Step: 19712...\n",
      "Epoch: 19/20... Step: 19968...\n",
      "Epoch: 19/20... Step: 20224...\n",
      "Epoch: 19/20... Step: 20480...\n",
      "Epoch: 20/20... Step: 20736...\n",
      "Epoch: 20/20... Step: 20992...\n",
      "Epoch: 20/20... Step: 21248...\n",
      "Epoch: 20/20... Step: 21504...\n"
     ]
    }
   ],
   "source": [
    "train(net, batch_size = 32, epochs=20, print_every=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, tkn, h=None):\n",
    "         \n",
    "    # tensor inputs\n",
    "    x = np.array([[word_to_idx[tkn]]])\n",
    "    inputs = torch.from_numpy(x).type(torch.LongTensor)\n",
    "  \n",
    "    # push to GPU\n",
    "    #inputs = inputs.cuda()\n",
    "\n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    # get the output of the model\n",
    "    out, h = net(inputs, h)\n",
    "\n",
    "    # get the token probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "\n",
    "    p = p.cpu()\n",
    "\n",
    "    p = p.numpy()\n",
    "    p = p.reshape(p.shape[1],)\n",
    "\n",
    "    # get indices of top 3 values\n",
    "    top_n_idx = p.argsort()[-3:][::-1]\n",
    "\n",
    "    # randomly select one of the three indices\n",
    "    sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
    "\n",
    "    # return the encoded value of the predicted char and the hidden state\n",
    "    return idx_to_word[sampled_token_index], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate text\n",
    "def sample(net, size, prime):\n",
    "        \n",
    "    # push to GPU\n",
    "    #net.cuda()\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    # batch size is 1\n",
    "    h = net.init_hidden(1)\n",
    "\n",
    "    toks = prime.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in prime.split():\n",
    "        token, h = predict(net, t, h)\n",
    "    \n",
    "    toks.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(size-1):\n",
    "        token, h = predict(net, toks[-1], h)\n",
    "        toks.append(token)\n",
    "\n",
    "    return ' '.join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with a fishstick bitch like i cant spend the presidito hola'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 10, \"with\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
